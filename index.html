<!DOCTYPE html>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js">
</script>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta property="og:title" content="T-Dex: Dexterity from Touch">
  <meta property="og:description" content="T-Dex: Dexterity from Touch">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="T-Dex: Dexterity from Touch">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="T-Dex: Dexterity from Touch">
  <meta name="twitter:description"
    content="We propose a new non-parametric dexterous manipulation framework with tactile and visual observations.">
  <link rel="stylesheet" href="css/simple-grid.css">
  <link rel="shortcut icon" href="mfiles/robotic-arm.png">

  <title>T-Dex: Dexterity from Touch</title>
</head>

<body>
  <div class="jumbotron">
    <div class="container">

      <div class="row"> <!-- - TODO --> 
        <div class="col-13 center">
          <h1>Dexterity from Touch: Self-Supervised Pre-Training of Tactile Representations with Robotic Play</h1>
        </div>
        <div class="col-3 hidden-sm"></div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://tactile-dexterity.github.io/" download>
            <h3 style="color: #F5A803">Paper - TODO</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://tactile-dexterity.github.io/" download>
            <h3 style="color: #F5A803">Data - TODO</h3>
          </a>
        </div>
        <div class="col-2 center">
          <a style="text-decoration: none" href="https://tactile-dexterity.github.io/" download>
            <h3 style="color: #F5A803">Code - TODO</h3>
          </a>
        </div>
      </div>

      <!-- Author names -->
      <div class="row">
        <div class="col-3 center">
          <a style="text-decoration: none" href="https://irmakguzey.github.io" download>
            <h3>Irmak Guzey</h3>
            <p>New York University</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none" href="https://bennevans.github.io/" download>
            <h3>Ben Evans</h3>
            <p>New York University</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none"  href="https://soumith.ch/" download>
            <h3>Soumith Chintala</h3>
            <p>Meta AI</p>
          </a>
        </div>
        <div class="col-3 center">
          <a style="text-decoration: none" href="https://lerrelpinto.com/" download>
            <h3>Lerrel Pinto</h3>
            <p>New York University</p>
          </a>
        </div>
      </div>
    </div>

      <!--Intro video-->
    <div class="container">
      <div class="intro-vid">
        <div class="col-12 center">
          <body>
            <iframe width="711" height="400" src="https://www.youtube.com/embed/JeI3WU37Ifk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
          </body>
        </div>
      </div>
      <br>
      <hr>
    </div>

    <!-- Abstract -->
    <div class="container"> 
      <div class="row">
        <div class="col-12 justify">
          <h3 class="center m-bottom">Summary</h2>
          <p>
            In this work we present T-Dex, a new approach for tactile-based dexterity, that operates
            in two phases. In the first phase, we collect 2.5 hours of play
            data, which is used to train self-supervised tactile encoders. 
            In the second phase, given a handful of
            demonstrations for a dexterous task, we learn non-parametric
            policies that combine the tactile observations with visual ones.
            Across five challenging dexterous tasks, we observe that our <b>tactile-
            based dexterity models outperform purely vision and torque-
            based models by an average of 1.7X</b>.
          </p>
        </div>
      </div>
      <br>
      <hr>
    </div>

      <!-- Policies -->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Policies</h2>
        </div>

        <p>
          We test our framework 
        </p>

        <div class="col-4">
            <img src="./mfiles/main_task/gamepad.gif"/>
            <h4 class="center">Joystick Movement</h4>
        </div>
        <div class="col-4">
            <img src="./mfiles/main_task/bottle_opening.gif"/>
            <h4 class="center">Bottle Cap Opening</h4>
        </div>
        <div class="col-4">
            <img src="./mfiles/main_task/book_opening.gif"/>
            <h4 class="center">Book Opening</h4>
        </div>
        <div class="col-4">
            <img src="./mfiles/main_task/bowl_picking.gif"/>
            <h4 class="center">Bowl Unstacking</h4>
        </div>
        <div class="col-4">
            <img src="./mfiles/main_task/cup_picking.gif"/>
            <h4 class="center">Cup Unstacking</h4>
        </div>
      </div>
      <br>
      <hr>
    </div>

    <!-- Generalization Experiments -->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Generalization Experiments</h2>
          <p>
            We observed that policies learned was able to generalize on structurally similar but visually different objects 
            in few of the tasks.
          </p>
        </div>

        <div class="col-6">
          <h4 class="center m-bottom">Cup Unstacking</h4>
          <video style="height: 100px" playsinline muted autoplay loop>
            <source src="./mfiles/generalization/cup_unstacking_gen.mp4" type="video/mp4">
          </video>
          <!-- <img src="./mfiles/generalization/cup_unstacking_gen.gif"/>  -->
          <p>
            Robot rollout for cup unstacking task on different types of cups.
          </p>
        </div>
        
        <div class="col-6">
          <h4 class="center m-bottom">Bowl Unstacking</h4>
          <!-- <img src="./mfiles/generalization/bowl_unstacking_gen.gif"/>  -->
          <video playsinline muted autoplay loop>
            <source src="./mfiles/generalization/bowl_unstacking_gen.mp4" type="video/mp4">
          </video>
          <p>
            Robot rollout for bowl unstacking task on different types of bowls.
          </p> 
        </div>
      </div>
      <hr>
    </div>
    
    <!--Method-->
    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Method</h2>
        </div>
        
        <div class="col-12">
          <p><a style="color: #00A2FF; font-weight: bold;">T-Dex</a> consists of two phases: First phase is collecting a tactile play data, where a teleoperator collects 2.5 hours of demonstrations
            and pretraining a tactile encoder from this play data. The second phase is retrieving policies by using nearest neighbor actions. 
          </p>
        </div>

        <div class="center img">
          <img src="./mfiles/tdex_algo.png" style="max-width:1000px;width:110%" frameborder="0"
          allowfullscreen></img>
        </div>

        <div class="col-12">
          <h3 class="center m-bottom">Collected Play Data</h3> 
          <p>
            Here, we show example tactile and visual data of the play data.
          </p>
        </div>
       
        <div class="col-6">
          <img src="./mfiles/play_data.gif"/>
          <p class="center">
            Example play data tasks done</p>
        </div>
        <div class="col-6">
          <img src="./mfiles/play_data_tactile.gif"/>
          <p class="center">
            Example play data tactile readings from top four activated tactile pads.
          </p>
        </div>
      </div>
      <hr>
    </div>

    <!-- <div class="row">
      <div class="center img">
        <img src="./mfiles/tdex_algo.png" style="max-width:1000px;width:60%" frameborder="0"
        allowfullscreen></img>
      </div>
    </div> -->

    <!-- <div class="container">
      <div class="row">
        <div class="col-12">
          <p>
            At each timestep, the closest example to the current observation is searched in the demonstration dataset.
            Representations for the demonstration buffer and the current state are received from the image encoder
              and the tactile encoder separately. And then concated with each other.
            The action associated with that closest example is then executed on the robot.
          </p>
        </div>
      </div>  
    </div> -->
      

    <!-- Play Data Collection Heading -->
    <!-- <div class="container">
        <div class="row">
          <div class="col-12">
              <h2 class="center m-bottom">Play Data Collection</h2> 
          </div>

          <div class="col-6">
              <p> 
                  Since it is hard to collect fine-grained dexterous demonstrations through teleoperation without the tactile feeling, we train a tactile encoder
                  through large amounts of play data. A teleoperator collects 2.5 hours of play demonstrations by using an Oculus headset and a buil-in Kinova joystick, 
                  shown in this figure.
              </p>
          </div>
          <div class="col-6">
              <img src="./mfiles/robot_setup.png"/>
          </div>

          <div class="col-12">
            <h3 class="center m-bottom">Example Play Data</h2> 
          </div>
          <div class="col-6">
            <img src="./mfiles/play_data.gif"/>
            <p class="center">
              Example play data tasks done</p>
          </div>
          <div class="col-6">
            <img src="./mfiles/play_data_tactile.gif"/>
            <p class="center">
              Example play data tactile readings from top four activated tactile pads.
            </p>
          </div>
        </div>
    </div>

    <br><br> -->

    <div class="container">
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Baseline comparisons</h2>
          <p>
            We observe that we require both the tactile and image observations combined together to successfully complete our tasks.
            We show the importance of our method decisions by comparing image only and tactile only neighbor matching
            policies to T-Dex.
          </p>

          <div class="col-6">
            <img src="./mfiles/baseline_comp/tdex_cup.gif"/>
            <h3 class="center">T-Dex</h3>
            <p class="center">
              <b>Success:</b> It successfully slides the inner cup.
            </p>
          </div>
          <div class="col-6">
            <img src="./mfiles/baseline_comp/image_only_cup.gif"/>
            <h3 class="center">Image Only NN Matching Policy</h3>
            <p class="center">
              <b>Failure:</b> It fails since the policy doesn't know how much force should be applied.
            </p>
          </div>
          <div class="col-6">
            <img src="./mfiles/baseline_comp/tactile_only_cup.gif"/>
            <h3 class="center">Tactile Only NN Matching Policy</h3>
            <p class="center">
              <b>Failure:</b> It fails since the policy doesn't know where the object is.
            </p>
          </div>
        </div>
      </div>
    </div>

  </div>



  <footer>
  </footer>

  <!-- <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.6.4/jquery.min.js"></script>
  <script>
    $(document).ready(function() {
      setInterval(planar_rotation_cycle, 10000);
      setInterval(object_flipping_cycle, 10000);
      setInterval(can_spinning_cycle, 10000);
      setInterval(failure_cycle, 10000);

      var planar_rotation_marker = 1;
      var object_flipping_marker = 1;
      var can_spinning_marker = 1;
      var failure_marker = 1;

      function planar_rotation_cycle() {
        planar_rotation_marker = (planar_rotation_marker % 6) + 1;
        $('#cr-' + planar_rotation_marker).prop('checked', true);
      }

      function object_flipping_cycle() {
        object_flipping_marker = (object_flipping_marker % 9) + 1;
        $('#dr-' + object_flipping_marker).prop('checked', true);
      }

      function can_spinning_cycle() {
        can_spinning_marker = (can_spinning_marker % 9) + 1;
        $('#er-' + can_spinning_marker).prop('checked', true);
      }

      function failure_cycle() {
        failure_marker = (failure_marker % 4) + 1;
        $('#fr-' + failure_marker).prop('checked', true);
      }
    }); -->
  <!-- </script> -->

</body>

</html>